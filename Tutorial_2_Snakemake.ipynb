{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cfa998-06b6-4b42-ae3a-b4e011750d31",
   "metadata": {},
   "source": [
    "# RNA-Seq Analysis Training Demo (Snakemake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126cb07-34ee-4780-838f-872015a882b3",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ea992-faa6-4705-8384-eb5d81f5daff",
   "metadata": {},
   "source": [
    "This short tutorial demonstrates how to run an RNA-Seq workflow using a prokaryotic data set. Steps in the workflow include read trimming, read QC, read mapping, and counting mapped reads per gene to quantitate gene expression. This tutorial uses a popular workflow manager called 'snakemake'. More information on snakemake can be found <a href=\"https://snakemake.readthedocs.io/en/stable/\">here</a>. Running the code in this tutorial will take approximately 12 minutes.\n",
    "\n",
    "![RNA-Seq workflow](images/rnaseq-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ab630-955d-43d1-bc43-c7b3e701ed04",
   "metadata": {
    "tags": []
   },
   "source": [
    "### STEP 1: Install mambaforge and snakemake\n",
    "First install mambaforge.\n",
    "\n",
    "We will use it to install snakemake, as well as create a snakemake environment using mambaforge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "682ddf88-e1d9-443f-a423-e1f85ff604a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 88.1M  100 88.1M    0     0  21.8M      0  0:00:04  0:00:04 --:--:-- 25.7M\n",
      "ERROR: File or directory already exists: '/home/jupyter/mambaforge'\n",
      "If you want to update an existing installation, use the -u option.\n",
      "\n",
      "                  __    __    __    __\n",
      "                 /  \\  /  \\  /  \\  /  \\\n",
      "                /    \\/    \\/    \\/    \\\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà/  /‚ñà‚ñà/  /‚ñà‚ñà/  /‚ñà‚ñà/  /‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "              /  / \\   / \\   / \\   / \\  \\____\n",
      "             /  /   \\_/   \\_/   \\_/   \\    o \\__,\n",
      "            / _/                       \\_____/  `\n",
      "            |/\n",
      "        ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó\n",
      "        ‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó\n",
      "        ‚ñà‚ñà‚ïî‚ñà‚ñà‚ñà‚ñà‚ïî‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ñà‚ñà‚ïî‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë\n",
      "        ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë\n",
      "        ‚ñà‚ñà‚ïë ‚ïö‚ïê‚ïù ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë ‚ïö‚ïê‚ïù ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë\n",
      "        ‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù\n",
      "\n",
      "        mamba (0.22.1) supported by @QuantStack\n",
      "\n",
      "        GitHub:  https://github.com/mamba-org/mamba\n",
      "        Twitter: https://twitter.com/QuantStack\n",
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "\n",
      "Looking for: ['snakemake']\n",
      "\n",
      "conda-forge/linux-64                                        Using cache\n",
      "conda-forge/noarch                                          Using cache\n",
      "bioconda/linux-64                                           Using cache\n",
      "bioconda/noarch                                             Using cache\n",
      "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
      "pkgs/main/linux-64 \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏\u001b[0m\u001b[33m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/noarch                                              No change\n",
      "pkgs/r/linux-64                                               No change\n",
      "pkgs/r/noarch                                                 No change\n",
      "[+] 0.1s\n",
      "pkgs/main/linux-64 \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏\u001b[0m\u001b[33m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/linux-64                                            No change\n",
      "\u001b[?25h\n",
      "Pinned packages:\n",
      "  - python 3.7.*\n",
      "\n",
      "\n",
      "Transaction\n",
      "\n",
      "  Prefix: /opt/conda\n",
      "\n",
      "  All requested packages already installed\n",
      "\n",
      "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\n",
    "!bash Mambaforge-$(uname)-$(uname -m).sh -b -p $HOME/mambaforge\n",
    "!$HOME/mambaforge/bin/mamba install -y -c conda-forge -c bioconda snakemake\n",
    "!cp /home/jupyter/mambaforge/bin/mamba /opt/conda/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d67299-cf08-4df0-99a3-aee8c761248a",
   "metadata": {},
   "source": [
    "### STEP 2: Create directories that will be used in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "773ba697-01f2-487d-b7ff-9885959b85cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/rnaseq-myco-notebook\n"
     ]
    }
   ],
   "source": [
    "!echo $PWD\n",
    "!mkdir -p data\n",
    "!mkdir -p data/raw_fastq\n",
    "!mkdir -p data/trimmed\n",
    "!mkdir -p data/reference\n",
    "!mkdir -p data/fastqc\n",
    "!mkdir -p envs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ce8d5-4b96-4e97-88ed-44e8e85f4fc0",
   "metadata": {},
   "source": [
    "### STEP 3: Copy FASTQ Files\n",
    "In order for this tutorial to run quickly, we will only analyze 50,000 reads from a sample from both sample groupsinstead of analyzing all the reads from all six samples. These files have been posted on a Google Storage Bucket that we made publicly accessible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db364248-e608-47c9-ab82-b5eab24ccfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 8452k  100 8452k    0     0   161M      0 --:--:-- --:--:-- --:--:--  165M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 8452k  100 8452k    0     0   180M      0 --:--:-- --:--:-- --:--:--  183M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 8452k  100 8452k    0     0   162M      0 --:--:-- --:--:-- --:--:--  165M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 8452k  100 8452k    0     0   157M      0 --:--:-- --:--:-- --:--:--  158M\n"
     ]
    }
   ],
   "source": [
    "!curl https://storage.googleapis.com/me-inbre-rnaseq-pipelinev2/data/raw_fastqSub/SRR13349122_1.fastq --output data/raw_fastq/SRR13349122_1.fastq\n",
    "!curl https://storage.googleapis.com/me-inbre-rnaseq-pipelinev2/data/raw_fastqSub/SRR13349122_2.fastq --output data/raw_fastq/SRR13349122_2.fastq\n",
    "!curl https://storage.googleapis.com/me-inbre-rnaseq-pipelinev2/data/raw_fastqSub/SRR13349128_1.fastq --output data/raw_fastq/SRR13349128_1.fastq\n",
    "!curl https://storage.googleapis.com/me-inbre-rnaseq-pipelinev2/data/raw_fastqSub/SRR13349128_2.fastq --output data/raw_fastq/SRR13349128_2.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec692c28-f549-43af-bbdf-3c4266fb59ae",
   "metadata": {},
   "source": [
    "### STEP 4: Copy reference transcriptome files that will be used by Salmon\n",
    "Salmon is a tool that aligns RNA-Seq reads to a set of transcripts rather than the entire genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "263c3e9c-4fc2-4945-a639-e1079e1aff5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 9599k  100 9599k    0     0   140M      0 --:--:-- --:--:-- --:--:--  142M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    14  100    14    0     0   1032      0 --:--:-- --:--:-- --:--:--  1076\n"
     ]
    }
   ],
   "source": [
    "!curl https://storage.googleapis.com/me-inbre-rnaseq-pipelinev2/data/reference/M_chelonae_transcripts.fasta --output data/reference/M_chelonae_transcripts.fasta\n",
    "!curl https://storage.googleapis.com/me-inbre-rnaseq-pipelinev2/data/reference/decoys.txt --output data/reference/decoys.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d17cb-dff6-45d3-9aef-3ec6203508f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### STEP 5: Copy data file for Trimmomatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7200cafc-c816-48f1-819c-dd3c2ce34de3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    95  100    95    0     0   7065      0 --:--:-- --:--:-- --:--:--  7307\n"
     ]
    }
   ],
   "source": [
    "!curl https://storage.googleapis.com/me-inbre-rnaseq-pipelinev2/config/TruSeq3-PE.fa --output TruSeq3-PE.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac668db-7851-418e-9b1e-0a2c4abbab6e",
   "metadata": {},
   "source": [
    "### STEP 6: Download data and config files that will be used in our snakemake environment\n",
    "\n",
    "Next download config files for our snakemake environment, as well as data files which we will analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a227a36a-a398-49f5-b497-7405574b018b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    67  100    67    0     0   5107      0 --:--:-- --:--:-- --:--:--  5153\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3561  100  3561    0     0   270k      0 --:--:-- --:--:-- --:--:--  289k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   106  100   106    0     0   7378      0 --:--:-- --:--:-- --:--:--  7571\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   106  100   106    0     0   8086      0 --:--:-- --:--:-- --:--:--  8153\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    86  100    86    0     0   5992      0 --:--:-- --:--:-- --:--:--  6142\n"
     ]
    }
   ],
   "source": [
    "# Copy config and data files\n",
    "\n",
    "!curl https://storage.googleapis.com/me-inbre-rnaseq-pipelinev2/config.yaml --output config.yaml\n",
    "!curl https://storage.googleapis.com/me-inbre-rnaseq-pipelinev2/snakefile --output snakefile\n",
    "!curl https://storage.googleapis.com/me-inbre-rnaseq-pipelinev2/envs/fastqc.yaml --output envs/fastqc.yaml\n",
    "!curl https://storage.googleapis.com/me-inbre-rnaseq-pipelinev2/envs/trimmomatic.yaml --output envs/trimmomatic.yaml\n",
    "!curl https://storage.googleapis.com/me-inbre-rnaseq-pipelinev2/envs/salmon.yaml --output envs/salmon.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64efb6e-cf92-4c6f-af76-d60c9a10ed59",
   "metadata": {},
   "source": [
    "#### Explanation of config files\n",
    "\n",
    "Snakemake is unique in that it uses config files to manage workflows in the form of 'yaml' files, as well as a 'snakefile'.\n",
    "\n",
    "Below is a brief example of some of the yaml config files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c648b302-f125-4dd6-b4e7-0d371e65b81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The config.yaml file contains our sample names:\n",
      "\n",
      " Config.yaml\n",
      "samples:\n",
      "    SRR13349122: SRR13349122\n",
      "    SRR13349128: SRR13349128\n",
      "\n",
      "\n",
      "The env folder contains information pertaining to packages to be used in the environment, \n",
      "as well as their version, for example, here is the 'envs/fastqc.yaml' file:\n",
      "\n",
      " Fastqc.Yaml\n",
      "channels:\n",
      "  - bioconda\n",
      "  - conda-forge\n",
      "  - defaults\n",
      "dependencies:\n",
      "  - fastqc ==0.11.9\n",
      "  - multiqc ==1.12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!printf \"The config.yaml file contains our sample names:\\n\\n Config.yaml\\n\"\n",
    "!cat config.yaml\n",
    "!printf \"\\n\\nThe env folder contains information pertaining to packages to be used in the environment, \\nas well as their version, for example, here is the 'envs/fastqc.yaml' file:\\n\\n Fastqc.Yaml\\n\"\n",
    "!cat envs/fastqc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c0273-c7f1-4aee-bdf3-43d5773cf2fa",
   "metadata": {},
   "source": [
    "### STEP 7: Run snakemake on our snakefile\n",
    "\n",
    "Aside from the .yaml config files which information about software, dependencies, and versions -- snakemake uses a snakefile which contains information about a workflow.\n",
    "\n",
    "This can be a powerful tool as it allows one to operate and think in terms of workflows instead of individual steps. \n",
    "\n",
    "Feel free to open the snakefile to look at it further. It is composed of 'rules' we have created.\n",
    "\n",
    "Snakefiles work largely based on inputs. For a given input, there is an associated 'rule' that runs.\n",
    "\n",
    "Snakefiles may take a while to get the idea of what's going on, but in simplest terms here we take an input of .fastq files, and based on the snakefile rules we created, those fastq files are run through the entire workflow of tutorial one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bee32318-33df-43b2-98bc-5eb091ceae59",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SRR13349122': 'SRR13349122', 'SRR13349128': 'SRR13349128'}\n",
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mUsing shell: /usr/bin/bash\u001b[0m\n",
      "\u001b[33mProvided cores: 4\u001b[0m\n",
      "\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n",
      "\u001b[33mJob stats:\n",
      "job                   count    min threads    max threads\n",
      "------------------  -------  -------------  -------------\n",
      "all                       1              1              1\n",
      "fastqc_trimmed            1              4              4\n",
      "multiqc_trimmed           1              1              1\n",
      "salmon_index              1              4              4\n",
      "salmon_quant_reads        2              2              2\n",
      "trimmomatic_pe_fq         2              4              4\n",
      "total                     8              1              4\n",
      "\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Wed Jun  1 15:37:46 2022]\u001b[0m\n",
      "\u001b[32mrule trimmomatic_pe_fq:\n",
      "    input: data/raw_fastq/SRR13349128_1.fastq, data/raw_fastq/SRR13349128_2.fastq\n",
      "    output: data/trimmed/SRR13349128_trimmed_1.fastq, data/trimmed/SRR13349128_trimmed_2.fastq, data/trimmed/SRR13349128_trimmed_1.unpaired.fastq, data/trimmed/SRR13349128_trimmed_2.unpaired.fastq\n",
      "    jobid: 2\n",
      "    reason: Forced execution\n",
      "    wildcards: sample=SRR13349128\n",
      "    threads: 4\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[33mActivating conda environment: .snakemake/conda/c5c104d5cc0c87027ce7881076c6b53e\u001b[0m\n",
      "/home/jupyter/rnaseq-myco-notebook/.snakemake/conda/c5c104d5cc0c87027ce7881076c6b53e/bin/trimmomatic: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n",
      "TrimmomaticPE: Started with arguments:\n",
      " -threads 4 data/raw_fastq/SRR13349128_1.fastq data/raw_fastq/SRR13349128_2.fastq data/trimmed/SRR13349128_trimmed_1.fastq data/trimmed/SRR13349128_trimmed_1.unpaired.fastq data/trimmed/SRR13349128_trimmed_2.fastq data/trimmed/SRR13349128_trimmed_2.unpaired.fastq ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36\n",
      "Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'\n",
      "ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\n",
      "Quality encoding detected as phred33\n",
      "Input Read Pairs: 50000 Both Surviving: 49851 (99.70%) Forward Only Surviving: 148 (0.30%) Reverse Only Surviving: 0 (0.00%) Dropped: 1 (0.00%)\n",
      "TrimmomaticPE: Completed successfully\n",
      "\u001b[32m[Wed Jun  1 15:37:48 2022]\u001b[0m\n",
      "\u001b[32mFinished job 2.\u001b[0m\n",
      "\u001b[32m1 of 8 steps (12%) done\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Wed Jun  1 15:37:48 2022]\u001b[0m\n",
      "\u001b[32mrule salmon_index:\n",
      "    input: data/reference/M_chelonae_transcripts.fasta\n",
      "    output: data/reference/transcriptome_index, data/reference/transcriptome_index/refseq.bin\n",
      "    jobid: 5\n",
      "    reason: Forced execution\n",
      "    threads: 4\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[33mActivating conda environment: .snakemake/conda/d4f33c5881f8b4f90819e2e3764f2eb5\u001b[0m\n",
      "Version Info: This is the most recent version of salmon.\n",
      "[2022-06-01 15:37:49.313] [jLog] [info] building index\n",
      "out : data/reference/transcriptome_index\n",
      "\u001b[00m[2022-06-01 15:37:49.313] [puff::index::jointLog] [info] Running fixFasta\n",
      "\u001b[00m\n",
      "[Step 1 of 4] : counting k-mers\n",
      "\n",
      "\u001b[35m[2022-06-01 15:37:49.520] [puff::index::jointLog] [warning] Removed 2 transcripts that were sequence duplicates of indexed transcripts.\n",
      "\u001b[00m\u001b[35m[2022-06-01 15:37:49.521] [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:49.521] [puff::index::jointLog] [info] Replaced 0 non-ATCG nucleotides\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:49.521] [puff::index::jointLog] [info] Clipped poly-A tails from 0 transcripts\n",
      "\u001b[00mwrote 4866 cleaned references\n",
      "\u001b[00m[2022-06-01 15:37:49.538] [puff::index::jointLog] [info] Filter size not provided; estimating from number of distinct k-mers\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:49.624] [puff::index::jointLog] [info] ntHll estimated 4966944 distinct k-mers, setting filter size to 2^27\n",
      "\u001b[00mThreads = 4\n",
      "Vertex length = 31\n",
      "Hash functions = 5\n",
      "Filter size = 134217728\n",
      "Capacity = 2\n",
      "Files: \n",
      "data/reference/transcriptome_index/ref_k31_fixed.fa\n",
      "--------------------------------------------------------------------------------\n",
      "Round 0, 0:134217728\n",
      "Pass\tFilling\tFiltering\n",
      "1\t1\t3\t\n",
      "2\t0\t0\n",
      "True junctions count = 10131\n",
      "False junctions count = 5086\n",
      "Hash table size = 15217\n",
      "Candidate marks count = 31083\n",
      "--------------------------------------------------------------------------------\n",
      "Reallocating bifurcations time: 0\n",
      "True marks count: 21315\n",
      "Edges construction time: 0\n",
      "--------------------------------------------------------------------------------\n",
      "Distinct junctions = 10131\n",
      "\n",
      "TwoPaCo::buildGraphMain:: allocated with scalable_malloc; freeing.\n",
      "TwoPaCo::buildGraphMain:: Calling scalable_allocation_command(TBBMALLOC_CLEAN_ALL_BUFFERS, 0);\n",
      "allowedIn: 13\n",
      "Max Junction ID: 10174\n",
      "seen.size():81401 kmerInfo.size():10175\n",
      "approximateContigTotalLength: 4947048\n",
      "counters for complex kmers:\n",
      "(prec>1 & succ>1)=8 | (succ>1 & isStart)=0 | (prec>1 & isEnd)=0 | (isStart & isEnd)=2\n",
      "contig count: 10353 element count: 5328123 complex nodes: 10\n",
      "# of ones in rank vector: 10352\n",
      "\u001b[00m[2022-06-01 15:37:54.708] [puff::index::jointLog] [info] Starting the Pufferfish indexing by reading the GFA binary file.\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.708] [puff::index::jointLog] [info] Setting the index/BinaryGfa directory data/reference/transcriptome_index\n",
      "\u001b[00msize = 5328123\n",
      "-----------------------------------------\n",
      "| Loading contigs | Time = 903.34 us\n",
      "-----------------------------------------\n",
      "size = 5328123\n",
      "-----------------------------------------\n",
      "| Loading contig boundaries | Time = 449.68 us\n",
      "-----------------------------------------\n",
      "Number of ones: 10352\n",
      "Number of ones per inventory item: 512\n",
      "Inventory entries filled: 21\n",
      "10352\n",
      "\u001b[00m[2022-06-01 15:37:54.716] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.717] [puff::index::jointLog] [info] contig count for validation: 10352\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.719] [puff::index::jointLog] [info] Total # of Contigs : 10352\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.719] [puff::index::jointLog] [info] Total # of numerical Contigs : 10352\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.719] [puff::index::jointLog] [info] Total # of contig vec entries: 16482\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.719] [puff::index::jointLog] [info] bits per offset entry 15\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.719] [puff::index::jointLog] [info] Done constructing the contig vector. 10353\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.721] [puff::index::jointLog] [info] # segments = 10352\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.721] [puff::index::jointLog] [info] total length = 5328123\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.722] [puff::index::jointLog] [info] Reading the reference files ...\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.754] [puff::index::jointLog] [info] positional integer width = 23\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.754] [puff::index::jointLog] [info] seqSize = 5328123\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.754] [puff::index::jointLog] [info] rankSize = 5328123\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.754] [puff::index::jointLog] [info] edgeVecSize = 0\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:54.755] [puff::index::jointLog] [info] num keys = 5017563\n",
      "\u001b[00mfor info, total work write each  : 2.331    total work inram from level 3 : 4.322  total work raw : 25.000 \n",
      "[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec\n",
      "Bitarray        26296128  bits (100.00 %)   (array + ranks )\n",
      "final hash             0  bits (0.00 %) (nb in final hash 0)\n",
      "\u001b[00m[2022-06-01 15:37:55.004] [puff::index::jointLog] [info] mphf size = 3.13474 MB\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:55.007] [puff::index::jointLog] [info] chunk size = 1332031\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:55.007] [puff::index::jointLog] [info] chunk 0 = [0, 1332031)\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:55.007] [puff::index::jointLog] [info] chunk 1 = [1332031, 2664062)\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:55.007] [puff::index::jointLog] [info] chunk 2 = [2664062, 3996093)\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:55.007] [puff::index::jointLog] [info] chunk 3 = [3996093, 5328093)\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:55.251] [puff::index::jointLog] [info] finished populating pos vector\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:55.251] [puff::index::jointLog] [info] writing index components\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:55.263] [puff::index::jointLog] [info] finished writing dense pufferfish index\n",
      "\u001b[00m[2022-06-01 15:37:55.266] [jLog] [info] done building index\n",
      "\u001b[32m[Wed Jun  1 15:37:55 2022]\u001b[0m\n",
      "\u001b[32mFinished job 5.\u001b[0m\n",
      "\u001b[32m2 of 8 steps (25%) done\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Wed Jun  1 15:37:55 2022]\u001b[0m\n",
      "\u001b[32mrule trimmomatic_pe_fq:\n",
      "    input: data/raw_fastq/SRR13349122_1.fastq, data/raw_fastq/SRR13349122_2.fastq\n",
      "    output: data/trimmed/SRR13349122_trimmed_1.fastq, data/trimmed/SRR13349122_trimmed_2.fastq, data/trimmed/SRR13349122_trimmed_1.unpaired.fastq, data/trimmed/SRR13349122_trimmed_2.unpaired.fastq\n",
      "    jobid: 1\n",
      "    reason: Forced execution\n",
      "    wildcards: sample=SRR13349122\n",
      "    threads: 4\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[33mActivating conda environment: .snakemake/conda/c5c104d5cc0c87027ce7881076c6b53e\u001b[0m\n",
      "/home/jupyter/rnaseq-myco-notebook/.snakemake/conda/c5c104d5cc0c87027ce7881076c6b53e/bin/trimmomatic: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n",
      "TrimmomaticPE: Started with arguments:\n",
      " -threads 4 data/raw_fastq/SRR13349122_1.fastq data/raw_fastq/SRR13349122_2.fastq data/trimmed/SRR13349122_trimmed_1.fastq data/trimmed/SRR13349122_trimmed_1.unpaired.fastq data/trimmed/SRR13349122_trimmed_2.fastq data/trimmed/SRR13349122_trimmed_2.unpaired.fastq ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36\n",
      "Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'\n",
      "ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\n",
      "Quality encoding detected as phred33\n",
      "Input Read Pairs: 50000 Both Surviving: 49870 (99.74%) Forward Only Surviving: 130 (0.26%) Reverse Only Surviving: 0 (0.00%) Dropped: 0 (0.00%)\n",
      "TrimmomaticPE: Completed successfully\n",
      "\u001b[32m[Wed Jun  1 15:37:56 2022]\u001b[0m\n",
      "\u001b[32mFinished job 1.\u001b[0m\n",
      "\u001b[32m3 of 8 steps (38%) done\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Wed Jun  1 15:37:56 2022]\u001b[0m\n",
      "\u001b[32mrule salmon_quant_reads:\n",
      "    input: data/trimmed/SRR13349122_trimmed_1.fastq, data/trimmed/SRR13349122_trimmed_2.fastq, data/reference/transcriptome_index\n",
      "    output: data/quants/SRR13349122_quant/quant.sf\n",
      "    jobid: 6\n",
      "    reason: Input files updated by another job: data/trimmed/SRR13349122_trimmed_1.fastq, data/reference/transcriptome_index, data/trimmed/SRR13349122_trimmed_2.fastq\n",
      "    wildcards: sample=SRR13349122\n",
      "    threads: 2\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[33mActivating conda environment: .snakemake/conda/d4f33c5881f8b4f90819e2e3764f2eb5\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Wed Jun  1 15:37:56 2022]\u001b[0m\n",
      "\u001b[32mrule salmon_quant_reads:\n",
      "    input: data/trimmed/SRR13349128_trimmed_1.fastq, data/trimmed/SRR13349128_trimmed_2.fastq, data/reference/transcriptome_index\n",
      "    output: data/quants/SRR13349128_quant/quant.sf\n",
      "    jobid: 7\n",
      "    reason: Input files updated by another job: data/trimmed/SRR13349128_trimmed_2.fastq, data/reference/transcriptome_index, data/trimmed/SRR13349128_trimmed_1.fastq\n",
      "    wildcards: sample=SRR13349128\n",
      "    threads: 2\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[33mActivating conda environment: .snakemake/conda/d4f33c5881f8b4f90819e2e3764f2eb5\u001b[0m\n",
      "Version Info: This is the most recent version of salmon.\n",
      "### salmon (selective-alignment-based) v1.8.0\n",
      "### [ program ] => salmon \n",
      "### [ command ] => quant \n",
      "### [ index ] => { data/reference/transcriptome_index }\n",
      "### [ libType ] => { SR }\n",
      "### [ unmatedReads ] => { data/trimmed/SRR13349122_trimmed_1.fastq }\n",
      "### [ threads ] => { 8 }\n",
      "### [ validateMappings ] => { }\n",
      "### [ output ] => { data/quants/SRR13349122_quant/ }\n",
      "Logs will be written to data/quants/SRR13349122_quant/logs\n",
      "Version Info: This is the most recent version of salmon.\n",
      "### salmon (selective-alignment-based) v1.8.0\n",
      "### [ program ] => salmon \n",
      "### [ command ] => quant \n",
      "### [ index ] => { data/reference/transcriptome_index }\n",
      "### [ libType ] => { SR }\n",
      "### [ unmatedReads ] => { data/trimmed/SRR13349128_trimmed_1.fastq }\n",
      "### [ threads ] => { 8 }\n",
      "### [ validateMappings ] => { }\n",
      "### [ output ] => { data/quants/SRR13349128_quant/ }\n",
      "Logs will be written to data/quants/SRR13349128_quant/logs\n",
      "\u001b[00m[2022-06-01 15:37:56.402] [jointLog] [info] setting maxHashResizeThreads to 8\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.402] [jointLog] [info] Fragment incompatibility prior below threshold.  Incompatible fragments will be ignored.\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.402] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.402] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.402] [jointLog] [info] parsing read library format\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.404] [jointLog] [info] setting maxHashResizeThreads to 8\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.404] [jointLog] [info] Fragment incompatibility prior below threshold.  Incompatible fragments will be ignored.\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.404] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.404] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.404] [jointLog] [info] parsing read library format\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.404] [jointLog] [info] There is 1 library.\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.404] [jointLog] [info] Loading pufferfish index\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.404] [jointLog] [info] Loading dense pufferfish index.\n",
      "\u001b[00m-----------------------------------------\n",
      "| Loading contig table | Time = 1.5922 ms\n",
      "-----------------------------------------\n",
      "size = 10353\n",
      "-----------------------------------------\n",
      "| Loading contig offsets | Time = 113.37 us\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading reference lengths | Time = 22.529 us\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading contig table | Time = 1.8426 ms\n",
      "-----------------------------------------\n",
      "size = 10353\n",
      "-----------------------------------------\n",
      "| Loading contig offsets | Time = 90.744 us\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading reference lengths | Time = 14.437 us\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading mphf table | Time = 2.2922 ms\n",
      "-----------------------------------------\n",
      "size = 5328123\n",
      "-----------------------------------------\n",
      "| Loading mphf table | Time = 2.3184 ms\n",
      "-----------------------------------------\n",
      "size = 5328123Number of ones: 10352\n",
      "\n",
      "Number of ones per inventory item: 512\n",
      "Number of ones: 10352\n",
      "Number of ones per inventory item: 512\n",
      "Inventory entries filled: 21\n",
      "Inventory entries filled: 21\n",
      "-----------------------------------------\n",
      "| Loading contig boundaries | Time = 8.0891 ms\n",
      "-----------------------------------------\n",
      "size = 5328123\n",
      "-----------------------------------------\n",
      "| Loading contig boundaries | Time = 7.9287 ms\n",
      "-----------------------------------------\n",
      "size = 5328123\n",
      "-----------------------------------------\n",
      "| Loading sequence | Time = 977.68 us\n",
      "-----------------------------------------\n",
      "size = 5017563\n",
      "-----------------------------------------\n",
      "| Loading sequence | Time = 866.8 us\n",
      "-----------------------------------------\n",
      "size = 5017563\n",
      "\u001b[00m[2022-06-01 15:37:56.403] [jointLog] [info] There is 1 library.\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.403] [jointLog] [info] Loading pufferfish index\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.403] [jointLog] [info] Loading dense pufferfish index.\n",
      "\u001b[00m-----------------------------------------\n",
      "| Loading positions | Time = 8.0968 ms\n",
      "-----------------------------------------\n",
      "size = 9683380\n",
      "-----------------------------------------\n",
      "| Loading positions | Time = 8.3038 ms\n",
      "-----------------------------------------\n",
      "size = 9683380\n",
      "-----------------------------------------\n",
      "| Loading reference sequence | Time = 1.4962 ms\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading reference accumulative lengths | Time = 35.899 us\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading reference sequence | Time = 1.561 ms\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading reference accumulative lengths | Time = 37.573 us\n",
      "-----------------------------------------\n",
      "\u001b[00m[2022-06-01 15:37:56.427] [jointLog] [info] done\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.427] [jointLog] [info] done\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.489] [jointLog] [info] Index contained 4866 targets\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.491] [jointLog] [info] Number of decoys : 1\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.491] [jointLog] [info] First decoy index : 4865 \n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.489] [jointLog] [info] Index contained 4866 targets\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.491] [jointLog] [info] Number of decoys : 1\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.491] [jointLog] [info] First decoy index : 4865 \n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.631] [jointLog] [info] Thread saw mini-batch with a maximum of 0.10% zero probability fragments\n",
      "[2022-06-01 15:37:56.636] [jointLog] [info] Thread saw mini-batch with a maximum of 0.18% zero probability fragments\n",
      "\u001b[00m\u001b[00m\u001b[00m[2022-06-01 15:37:56.631] [jointLog] [info] Thread saw mini-batch with a maximum of 0.10% zero probability fragments\n",
      "\u001b[00m\u001b[00m\u001b[00m[2022-06-01 15:37:56.638] [jointLog] [info] Thread saw mini-batch with a maximum of 0.32% zero probability fragments\n",
      "\u001b[00m[2022-06-01 15:37:56.631] [jointLog] [info] Thread saw mini-batch with a maximum of 0.14% zero probability fragments\n",
      "\u001b[00m\u001b[00m\u001b[00m[2022-06-01 15:37:56.639] [jointLog] [info] Thread saw mini-batch with a maximum of 0.41% zero probability fragments\n",
      "\u001b[00m[2022-06-01 15:37:56.634] [jointLog] [info] Thread saw mini-batch with a maximum of 0.18% zero probability fragments\n",
      "\u001b[00m[2022-06-01 15:37:56.641] [jointLog] [info] Thread saw mini-batch with a maximum of 0.26% zero probability fragments\n",
      "\u001b[00m\u001b[00m\u001b[00m[2022-06-01 15:37:56.642] [jointLog] [info] Thread saw mini-batch with a maximum of 0.28% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.634] [jointLog] [info] Thread saw mini-batch with a maximum of 0.20% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.635] [jointLog] [info] Thread saw mini-batch with a maximum of 0.22% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.636] [jointLog] [info] Thread saw mini-batch with a maximum of 0.20% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.643] [jointLog] [info] Thread saw mini-batch with a maximum of 0.26% zero probability fragments\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TBB Warning: The number of workers is currently limited to 3. The request for 7 workers is ignored. Further requests for more workers will be silently ignored until the limit changes.\n",
      "\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.660] [jointLog] [info] Computed 851 rich equivalence classes for further processing\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.647] [jointLog] [info] Thread saw mini-batch with a maximum of 0.18% zero probability fragments\n",
      "[2022-06-01 15:37:56.660] [jointLog] [info] Counted 1907 total reads in the equivalence classes \n",
      "\u001b[00m\u001b[00m\u001b[00m\u001b[00m[2022-06-01 15:37:56.662] [jointLog] [info] Number of mappings discarded because of alignment score : 81\n",
      "[2022-06-01 15:37:56.648] [jointLog] [info] Thread saw mini-batch with a maximum of 0.26% zero probability fragments\n",
      "\u001b[00m\u001b[00m\u001b[00m\u001b[00m[2022-06-01 15:37:56.665] [jointLog] [info] Computed 1145 rich equivalence classes for further processing\n",
      "[2022-06-01 15:37:56.662] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 160\n",
      "\u001b[00m\u001b[00m\u001b[00m\u001b[00m[2022-06-01 15:37:56.662] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 151\n",
      "[2022-06-01 15:37:56.665] [jointLog] [info] Counted 2816 total reads in the equivalence classes \n",
      "\u001b[00m\u001b[00m\u001b[00m[2022-06-01 15:37:56.662] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 0\n",
      "\u001b[00m\u001b[35m[2022-06-01 15:37:56.663] [jointLog] [warning] Only 1907 fragments were mapped, but the number of burn-in fragments was set to 5000000.\n",
      "The effective lengths have been computed using the observed mappings.\n",
      "\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.663] [jointLog] [info] Mapping rate = 3.8254%\n",
      "\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.663] [jointLog] [info] finished quantifyLibrary()\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.663] [jointLog] [info] Starting optimizer\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.666] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.667] [jointLog] [info] iteration = 0 | max rel diff. = 0.996299\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "TBB Warning: The number of workers is currently limited to 3. The request for 7 workers is ignored. Further requests for more workers will be silently ignored until the limit changes.\n",
      "\n",
      "\u001b[00m[2022-06-01 15:37:56.669] [jointLog] [info] Number of mappings discarded because of alignment score : 191\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.669] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 262\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.669] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 227\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.669] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 0\n",
      "\u001b[00m\u001b[35m[2022-06-01 15:37:56.670] [jointLog] [warning] Only 2816 fragments were mapped, but the number of burn-in fragments was set to 5000000.\n",
      "The effective lengths have been computed using the observed mappings.\n",
      "\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.670] [jointLog] [info] Mapping rate = 5.64668%\n",
      "\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.670] [jointLog] [info] finished quantifyLibrary()\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.670] [jointLog] [info] Starting optimizer\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.673] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.673] [jointLog] [info] iteration = 0 | max rel diff. = 0.98942\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.700] [jointLog] [info] iteration = 100 | max rel diff. = 0\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.701] [jointLog] [info] Finished optimizer\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.701] [jointLog] [info] writing output \n",
      "\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.713] [jointLog] [info] iteration = 100 | max rel diff. = 0\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.713] [jointLog] [info] Finished optimizer\n",
      "\u001b[00m\u001b[00m[2022-06-01 15:37:56.713] [jointLog] [info] writing output \n",
      "\n",
      "\u001b[00m\u001b[32m[Wed Jun  1 15:37:57 2022]\u001b[0m\n",
      "\u001b[32mFinished job 6.\u001b[0m\n",
      "\u001b[32m4 of 8 steps (50%) done\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Wed Jun  1 15:37:57 2022]\u001b[0m\n",
      "\u001b[32mrule multiqc_trimmed:\n",
      "    input: data/trimmed/SRR13349122_trimmed_1.fastq, data/trimmed/SRR13349128_trimmed_1.fastq, data/trimmed/SRR13349122_trimmed_2.fastq, data/trimmed/SRR13349128_trimmed_2.fastq\n",
      "    output: multiqc_report.html\n",
      "    jobid: 4\n",
      "    reason: Input files updated by another job: data/trimmed/SRR13349122_trimmed_2.fastq, data/trimmed/SRR13349128_trimmed_1.fastq, data/trimmed/SRR13349122_trimmed_1.fastq, data/trimmed/SRR13349128_trimmed_2.fastq\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[33mActivating conda environment: .snakemake/conda/7c2a231706a71b566b29f65dd3d644bf\u001b[0m\n",
      "\u001b[32m[Wed Jun  1 15:37:57 2022]\u001b[0m\n",
      "\u001b[32mFinished job 7.\u001b[0m\n",
      "\u001b[32m5 of 8 steps (62%) done\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\n",
      "  \u001b[34m/\u001b[0m\u001b[32m/\u001b[0m\u001b[31m/\u001b[0m \u001b]8;id=995395;https://multiqc.info\u001b\\\u001b[1mMultiQC\u001b[0m\u001b]8;;\u001b\\ üîç \u001b[2m| v1.12 (9627e8a)\u001b[0m\n",
      "\n",
      "\u001b[34m|           multiqc\u001b[0m | Search path : /home/jupyter/rnaseq-myco-notebook/data/fastqc\n",
      "\u001b[2K\u001b[34m|\u001b[0m         \u001b[34msearching\u001b[0m | \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[35m100%\u001b[0m \u001b[32m32/32\u001b[0m  mmed_1_fastqc.html\u001b[0m.html\u001b[0m\n",
      "\u001b[?25h\u001b[34m|            fastqc\u001b[0m | Found 16 reports\n",
      "\u001b[34m|           multiqc\u001b[0m | Compressing plot data\n",
      "\u001b[34m|           multiqc\u001b[0m | \u001b[33mDeleting    : multiqc_data   (-f was specified)\u001b[0m\n",
      "\u001b[34m|           multiqc\u001b[0m | Report      : multiqc_report.html\n",
      "\u001b[34m|           multiqc\u001b[0m | Data        : multiqc_data\n",
      "\u001b[34m|           multiqc\u001b[0m | MultiQC complete\n",
      "\u001b[32m[Wed Jun  1 15:38:11 2022]\u001b[0m\n",
      "\u001b[32mFinished job 4.\u001b[0m\n",
      "\u001b[32m6 of 8 steps (75%) done\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Wed Jun  1 15:38:11 2022]\u001b[0m\n",
      "\u001b[32mrule fastqc_trimmed:\n",
      "    input: data/trimmed/SRR13349122_trimmed_1.fastq, data/trimmed/SRR13349128_trimmed_1.fastq, data/trimmed/SRR13349122_trimmed_2.fastq, data/trimmed/SRR13349128_trimmed_2.fastq\n",
      "    output: data/fastqc/SRR13349122_trimmed_1_fastqc.html, data/fastqc/SRR13349128_trimmed_1_fastqc.html, data/fastqc/SRR13349122_trimmed_2_fastqc.html, data/fastqc/SRR13349128_trimmed_2_fastqc.html\n",
      "    jobid: 3\n",
      "    reason: Input files updated by another job: data/trimmed/SRR13349122_trimmed_2.fastq, data/trimmed/SRR13349128_trimmed_1.fastq, data/trimmed/SRR13349122_trimmed_1.fastq, data/trimmed/SRR13349128_trimmed_2.fastq\n",
      "    threads: 4\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[33mActivating conda environment: .snakemake/conda/7c2a231706a71b566b29f65dd3d644bf\u001b[0m\n",
      "Started analysis of SRR13349122_trimmed_1.fastq\n",
      "Approx 5% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 10% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 15% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 20% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 25% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 30% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 35% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 40% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 45% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 50% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 55% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 60% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 65% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 70% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 75% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 80% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 85% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 90% complete for SRR13349122_trimmed_1.fastq\n",
      "Started analysis of SRR13349128_trimmed_1.fastq\n",
      "Approx 95% complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 5% complete for SRR13349128_trimmed_1.fastq\n",
      "Analysis complete for SRR13349122_trimmed_1.fastq\n",
      "Approx 10% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 15% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 20% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 25% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 30% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 35% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 40% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 45% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 50% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 55% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 60% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 65% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 70% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 75% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 80% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 85% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 90% complete for SRR13349128_trimmed_1.fastq\n",
      "Approx 95% complete for SRR13349128_trimmed_1.fastq\n",
      "Analysis complete for SRR13349128_trimmed_1.fastq\n",
      "Started analysis of SRR13349122_trimmed_2.fastq\n",
      "Approx 5% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 10% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 15% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 20% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 25% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 30% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 35% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 40% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 45% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 50% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 55% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 60% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 65% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 70% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 75% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 80% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 85% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 90% complete for SRR13349122_trimmed_2.fastq\n",
      "Approx 95% complete for SRR13349122_trimmed_2.fastq\n",
      "Analysis complete for SRR13349122_trimmed_2.fastq\n",
      "Started analysis of SRR13349128_trimmed_2.fastq\n",
      "Approx 5% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 10% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 15% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 20% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 25% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 30% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 35% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 40% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 45% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 50% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 55% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 60% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 65% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 70% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 75% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 80% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 85% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 90% complete for SRR13349128_trimmed_2.fastq\n",
      "Approx 95% complete for SRR13349128_trimmed_2.fastq\n",
      "Analysis complete for SRR13349128_trimmed_2.fastq\n",
      "\u001b[32m[Wed Jun  1 15:38:20 2022]\u001b[0m\n",
      "\u001b[32mFinished job 3.\u001b[0m\n",
      "\u001b[32m7 of 8 steps (88%) done\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Wed Jun  1 15:38:20 2022]\u001b[0m\n",
      "\u001b[32mlocalrule all:\n",
      "    input: data/trimmed/SRR13349122_trimmed_1.fastq, data/trimmed/SRR13349128_trimmed_1.fastq, data/fastqc/SRR13349122_trimmed_1_fastqc.html, data/fastqc/SRR13349128_trimmed_1_fastqc.html, multiqc_report.html, data/reference/transcriptome_index/refseq.bin, data/quants/SRR13349122_quant/quant.sf, data/quants/SRR13349128_quant/quant.sf\n",
      "    jobid: 0\n",
      "    reason: Input files updated by another job: data/quants/SRR13349128_quant/quant.sf, data/trimmed/SRR13349122_trimmed_1.fastq, data/fastqc/SRR13349128_trimmed_1_fastqc.html, data/fastqc/SRR13349122_trimmed_1_fastqc.html, data/quants/SRR13349122_quant/quant.sf, data/reference/transcriptome_index/refseq.bin, data/trimmed/SRR13349128_trimmed_1.fastq, multiqc_report.html\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Wed Jun  1 15:38:20 2022]\u001b[0m\n",
      "\u001b[32mFinished job 0.\u001b[0m\n",
      "\u001b[32m8 of 8 steps (100%) done\u001b[0m\n",
      "\u001b[33mComplete log: .snakemake/log/2022-06-01T153745.040275.snakemake.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!conda config --set channel_priority strict\n",
    "!snakemake --cores --use-conda --forceall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9deb0a-1030-4839-aa16-37c3b32a2c87",
   "metadata": {},
   "source": [
    "### STEP 8: Report the top 10 most highly expressed genes in the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f9bd2-dbd2-467f-a9b6-313e63ad304b",
   "metadata": {},
   "source": [
    "Top 10 most highly expressed genes in the wild-type sample. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7776f671-30a0-4ba8-a9cc-e3434d40cc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BB28_RS20665\t1293\t1043.000\t5447.071698\t55.000\n",
      "BB28_RS03075\t1626\t1376.000\t3077.869008\t41.000\n",
      "BB28_RS07370\t9255\t9005.000\t424.426717\t37.000\n",
      "BB28_RS20690\t10377\t10127.000\t367.203150\t36.000\n",
      "BB28_RS19405\t3948\t3698.000\t1005.588509\t36.000\n",
      "BB28_RS18585\t1305\t1055.000\t2741.512828\t28.000\n",
      "BB28_RS20685\t7731\t7481.000\t372.811085\t27.000\n",
      "BB28_RS19310\t1194\t944.000\t2626.176789\t24.000\n",
      "BB28_RS22260\t1836\t1586.000\t1497.991546\t23.000\n",
      "BB28_RS09805\t1437\t1187.000\t2001.528725\t23.000\n",
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!sort -nrk 5,5 data/quants/SRR13349122_quant/quant.sf | head -10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678efdde-1782-4481-9240-054c34528163",
   "metadata": {},
   "source": [
    "Top 10 most highly expressed genes in the double lysogen sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ceee200-b741-4954-b950-85edec98eb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BB28_RS20665\t1293\t1043.000\t16640.051824\t107.000\n",
      "BB28_RS18585\t1305\t1055.000\t5381.096618\t35.000\n",
      "BB28_RS13330\t2790\t2540.000\t2107.343957\t33.000\n",
      "BB28_RS14905\t972\t722.000\t6065.711819\t27.000\n",
      "BB28_RS22260\t1836\t1586.000\t1943.146845\t19.000\n",
      "BB28_RS18315\t1767\t1517.000\t2031.529926\t19.000\n",
      "BB28_RS20685\t7731\t7481.000\t368.590781\t17.000\n",
      "BB28_RS20690\t10377\t10127.000\t240.251247\t15.000\n",
      "BB28_RS03075\t1626\t1376.000\t1768.186333\t15.000\n",
      "BB28_RS11085\t1278\t1028.000\t2208.971570\t14.000\n",
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!sort -nrk 5,5 data/quants/SRR13349128_quant/quant.sf | head -10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50169f62-e707-4d84-b301-ded51a704130",
   "metadata": {},
   "source": [
    "### STEP 9: Report the expression of a putative acyl-ACP desaturase (BB28_RS16545) that was downregulated in the double lysogen relative to wild-type\n",
    "A acyl-transferase was reported to be downregulated in the double lysogen as shown in the table of the top 20 upregulated and downregulated genes from the paper describing the study.\n",
    "![RNA-Seq workflow](images/table-cushman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3794b0-a477-45fa-aa51-4414d7671441",
   "metadata": {},
   "source": [
    "Use `grep` to report the expression in the wild-type sample. The fields in the Salmon `quant.sf` file are as follows. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3cb9340-682b-4177-837d-7d803a9775a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BB28_RS16545\t987\t737.000\t560.631139\t4.000\n"
     ]
    }
   ],
   "source": [
    "!grep 'BB28_RS16545' data/quants/SRR13349122_quant/quant.sf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba6401-261d-43e9-b831-ef76122da623",
   "metadata": {},
   "source": [
    "Use `grep` to report the expression in the double lysogen sample. The fields in the Salmon `quant.sf` file are as follows. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "745ea1c5-79d3-481c-9359-6e0a93b9a286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BB28_RS16545\t987\t737.000\t220.083619\t1.000\n"
     ]
    }
   ],
   "source": [
    "!grep 'BB28_RS16545' data/quants/SRR13349128_quant/quant.sf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa372596-3377-4443-a325-e68bfa44c079",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"workflow\">Additional Workflows</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0bc9c-ff37-4ca1-b094-68f591d993a6",
   "metadata": {},
   "source": [
    "Now that you have read counts per gene, feel free to explore the R workflow which creates plots and analyses using these readcount files, or try other alternate workflows for creating read count files, such as the standard short or extended tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a552e6-e8fb-49da-a223-34c370054910",
   "metadata": {},
   "source": [
    "\n",
    "[Workflow One:](Tutorial_1.ipynb) A short introduction to downloading and mapping sequences to a transcriptome using Trimmomatic and Salmon. Here is a link to the YouTube video demonstrating the tutorial: <https://www.youtube.com/watch?v=NG1U7D4l31o&t=26s>.\n",
    "\n",
    "[Workflow One (Extended):](Tutorial_1B_Extended.ipynb) An extended version of workflow one. Once you have got your feet wet, you can retry workflow one with this extended version that covers the entire dataset, and includes elaboration such as using SRA tools for sequence downloading, and examples of running batches of fastq files through the pipeline. This workflow may take around an hour to run.\n",
    "\n",
    "[Workflow One (Using Snakemake):](Tutorial_2_Snakemake.ipynb) Using snakemake to run workflow one.\n",
    "\n",
    "[Workflow Two (DEG Analysis):](Tutorial_3_DEG_Analysis.ipynb) Using Deseq2 and R to conduct clustering and differential gene expression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf924fb-4301-43ee-b3d5-377c5623c146",
   "metadata": {},
   "source": [
    "![RNA-Seq workflow](images/RNA-Seq_Notebook_Homepage.png)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-1.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-1:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
